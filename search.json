[
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland."
  },
  {
    "objectID": "content/customizing.html",
    "href": "content/customizing.html",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#examples",
    "href": "content/customizing.html#examples",
    "title": "Customization",
    "section": "Examples",
    "text": "Examples\nLooking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFaye lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory."
  },
  {
    "objectID": "content/diffusion.html",
    "href": "content/diffusion.html",
    "title": "Diffusion Models",
    "section": "",
    "text": "Here is the general idea on how nonequilibrium thermodynamics was used to create an algorithm for generative modelling:\n\nConvert images to a simple noise distribution\nReverse the process by using the distribution to create new images.\n\nTo understand how this works, the article titled “The Physics Principle That Inspired Modern AI Art” described the process best. Suppose we have a grayscale image consisting of two pixels, with different grayscale values. If we plot the 2D plot of this, with each axis representing the grayscale values, we get a single point.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n\nnum1 = np.random.randint(0,256)\nnum2 = np.random.randint(0,256)\nx = np.array([[num1,num2]])\nplt.figure(figsize = (12,6))\n\nplt.subplot(121)\nplt.imshow(x, vmin = 0,vmax = 255, cmap = 'gray')\nplt.xticks([])\nplt.yticks([])\nplt.title(\"grayscale values: \" + str(num1) + \" and \" + str(num2))\n\nplt.subplot(122)\nplt.scatter([num1],[num2])\nplt.xlabel('image grayscale 1 value')\nplt.ylabel('image grayscale 2 value')\nplt.xlim(0,256)\nplt.ylim(0,256)\nplt.show()\n\n\n\n\n\nIf we have let’s say 20 grayscale images consisting of two pixels each and plot the grayscale values in the 2D plot, we notice that some points are closer to each other than other points. Then we can actually use this 2D plot to generate a 2D probability distribution function such that if we want to generate a new image, we just select a point from our distribution function and the result will give us the values of the pixels of our image, which is a combination of all the images from where we got the probability distribution from, with the images that clustered together having the highest similarity.\n\n\nCode\nnum1_list = []\nnum2_list = []\n\nplt.figure(figsize = (12*1,6*1.5))\n\nfor i in range(20):\n    plt.subplot(5,4,i+1)\n    num1 = np.random.randint(0,256)\n    num2 = np.random.randint(0,256)\n    num1_list.append(num1)\n    num2_list.append(num2)\n    x = np.array([[num1,num2]])\n    plt.imshow(x, vmin = 0,vmax = 255, cmap = 'gray')\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(\"grayscale values: \" + str(num1) + \" and \" + str(num2), fontsize = 8)\nplt.show()\n\nplt.figure(figsize = (6,6))\nplt.scatter(num1_list,num2_list)\nplt.xlabel('image grayscale 1 value')\nplt.ylabel('image grayscale 2 value')\nplt.xlim(0,256)\nplt.ylim(0,256)\nplt.show()\n\n\n\n\n\n\n\n\nIf we extend this to an actual image, we get an N-D plot depending on the number of pixels of our image, with the simplified explanation still holding true. And the challenge now for researchers is how to generate this complicated probability distribution function.\nPitfall encoutered: Images formed just looked like blobs, but still cool! The general ‘shape’ was there, but in blob form."
  },
  {
    "objectID": "content/diffusion.html#model-based-on-estimating-gradients-of-data-distribution",
    "href": "content/diffusion.html#model-based-on-estimating-gradients-of-data-distribution",
    "title": "Diffusion Models",
    "section": "Model based on Estimating Gradients of Data Distribution",
    "text": "Model based on Estimating Gradients of Data Distribution\nInstead of estimating the probability distribution of the data, this model estimated the gradient of the distribution. Here is a quick summary of how the model was done:\n\nPerturb image with random Gaussian noise of various magnitude\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\n\nimg = cv2.imread('test_image.jpg')[...,::-1]/255.0\nnoise =  np.random.normal(loc=0, scale=1, size=img.shape)\nlevels = [0,0.2,0.4,0.6,0.8]\n\nplt.figure(figsize = (3*5,4))\nnoise_used = (noise - noise.min())/(noise.max()-noise.min())\nplt.subplot(1,6,1)\nplt.imshow(noise_used)\nplt.title('noise')\nplt.xticks([])\nplt.yticks([])\nfor i,j in enumerate(levels):\n    noisy_img = np.clip((img + noise*j),0,1)\n    plt.subplot(1,6,i+2)\n    plt.imshow(noisy_img)\n    plt.title('noise = '+ str(levels[i]))\n    plt.xticks([])\n    plt.yticks([])\nplt.show()\n\n\n\n\n\n\nUse trained neural network to predict the original image using the gradient of the distribution of the noisy images (process looks like the inverse of the plots above).\n\nPitfall encountered: Though the images predicted are of good quality, the computational time it took was very long."
  },
  {
    "objectID": "content/diffusion.html#denoising-diffusion-probabilistic-models",
    "href": "content/diffusion.html#denoising-diffusion-probabilistic-models",
    "title": "Diffusion Models",
    "section": "Denoising Diffusion Probabilistic Models",
    "text": "Denoising Diffusion Probabilistic Models\nIn this final paper, the two models discussed above were combined. What happens here is that from a noisy blob of data, we get a recognizable image. What happens in this model is that, there is a forward and a backward process.\n\n\nCode\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n\nfrom typing import Any, List\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom sklearn.datasets import make_swiss_roll\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\nfrom pydantic import BaseModel\n\nfrom diffusers import DDPM\nfrom models import BasicDiscreteTimeModel\nfrom IPython.display import Image\n\nclass TrainResult(BaseModel):\n    losses: List[int]\n    samples: List[Any]\n\n\ndef train(\n    model: nn.Module,\n    ddpm: DDPM,\n    batch_size: int = 128,\n    n_epochs: int = 400,\n    sample_size: int = 512,\n    steps_between_sampling: int = 20,\n    seed: int = 42,\n) -> TrainResult:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    assert batch_size > 0 and steps_between_sampling > 0 and sample_size > 0\n\n    N = 1 << 10\n    X = make_swiss_roll(n_samples=N, noise=1e-1)[0][:, [0, 2]] / 10.0\n\n    optim = torch.optim.Adam(model.parameters(), 1e-3)\n\n    losses: List[float] = []\n    samples: List[Any] = []\n    step = 0\n    avg_loss = None  # exponential moving average\n    for _ in range(n_epochs):\n        ids = np.random.choice(N, N, replace=False)\n        for i in range(0, len(ids), batch_size):\n            x = torch.tensor(X[ids[i : i + batch_size]], dtype=torch.float32)\n            optim.zero_grad()\n            loss = ddpm.diffusion_loss(model, x)\n            loss.backward()\n            optim.step()\n\n            losses.append(loss.item())\n            if avg_loss is None:\n                avg_loss = losses[-1]\n            else:\n                avg_loss = 0.95 * avg_loss + 0.05 * losses[-1]\n            if not step % steps_between_sampling:\n                samples.append(ddpm.sample(model, n_samples=sample_size))\n            step += 1\n    return TrainResult(losses=losses, samples=samples)\n\n\ndef animate(samples: List[Any], save: bool = True):\n    fig, ax = plt.subplots(figsize=(8, 5))\n    ax.set(xlim=(-2.0, 2.0), ylim=(-2.0, 2.0))\n    scat = ax.scatter(*samples[0].detach().numpy().T, c=\"k\", alpha=0.3)\n\n    def animate(i):\n        scat.set_offsets(samples[i].detach().numpy())\n\n    anim = animation.FuncAnimation(fig, animate, interval=100, frames=len(samples) - 1)\n    if save:\n        anim.save(filename=\"animation.gif\", writer=animation.PillowWriter(fps=5))\n    #return anim\n\n\ndef main(\n    n_steps: int = 100,\n    d_model: int = 128,\n    n_layers: int = 2,\n    batch_size: int = 128,\n    n_epochs: int = 400,\n    sample_size: int = 512,\n    steps_between_sampling: int = 20,\n    seed: int = 42,\n):\n    #print(\"Creating model\")\n    model = BasicDiscreteTimeModel(d_model=d_model, n_layers=n_layers)\n    ddpm = DDPM(n_steps=n_steps)\n\n    #print(\"Training\")\n    result = train(\n        model=model,\n        ddpm=ddpm,\n        batch_size=batch_size,\n        n_epochs=n_epochs,\n        sample_size=sample_size,\n        steps_between_sampling=steps_between_sampling,\n        seed=seed,\n    )\n    path = \"animation.gif\"\n    animate(result.samples)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\nY\n\n\n\nFigure 1: Example of backward process\n\n\nTo explain the process of this paper, here are the important steps that I have noted and I think are important:\n\nStarting with the “real images”, we can get a real data distribution from these set of images. We can then sample this dataset to get an image for example.\nDefine forward diffusion process, that is, adding a Gaussian noise at each time step t, with a mean and variance value.\nUse a neural network that is trained using the images with gaussian noise added to them and basically learn (represent) the mean and variance of the probability distribution. In the original paper, the variance was fixed and only the mean changes."
  },
  {
    "objectID": "content/diffusion.html#sample-images-with-their-prompts",
    "href": "content/diffusion.html#sample-images-with-their-prompts",
    "title": "Diffusion Models",
    "section": "Sample Images with their prompts",
    "text": "Sample Images with their prompts\n\n\n\n\n\n\nA pikachu fine dining with a view to the Eiffel Tower\n\n\n\n\n\n\n\nA pikachu fine dining with a view to the Eiffel Tower\n\n\n\n\n\n\n\n\n\nA pikachu fine dining with a view to the Eiffel Tower\n\n\n\n\n\n\n\nA pikachu fine dining with a view to the Eiffel Tower"
  },
  {
    "objectID": "content/hyperspectral_trial.html",
    "href": "content/hyperspectral_trial.html",
    "title": "Physics 312 - Advanced Mathematical Physics II",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.io\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom tqdm import tqdm\npd.options.mode.chained_assignment = None \n%matplotlib inline\n\n\ndata = scipy.io.loadmat('munsell400_700_5.mat')\n\nmunsell = data['munsell']\nwavelength = np.arange(400,700+5,5)\n\ndf_fluorescent = pd.read_excel(\"Fluorescents.xls\", skiprows = 1)\ndf_fluorescent.rename(columns={'Unnamed: 0':'wavelength'}, inplace=True)\n\ndf_fluorescentF1 = df_fluorescent[['wavelength', 'F1']]\ndf_fluorescentF1.rename(columns = {'F1':'illumination'}, inplace = True)\n\ndf_fluorescentF1 = df_fluorescentF1[(df_fluorescentF1['wavelength'] >= 400) & (df_fluorescentF1['wavelength'] <= 700)]\ndf_fluorescentF1 = df_fluorescentF1.reset_index(inplace=False)\n\n\ncolor_signal = []\nfor i,j in enumerate(munsell):\n    color_signal.append(j * df_fluorescentF1.illumination[i])\ncolor_signal = np.array(color_signal)\n\n\npca = PCA(len(color_signal))\npca.fit(color_signal.T)\n\nmean = pca.mean_\nevecs = pca.components_\nevals = pca.explained_variance_ratio_\n\nexplained = pca.explained_variance_ratio_.cumsum() * 100\n\n\naccuracy = 95\nk = np.argmax(explained>accuracy)\nprint(\"Number of components explaining 95% variance: \"+ str(k+1))\n\nNumber of components explaining 95% variance: 3\n\n\n\nplt.figure(figsize = (9*3,6))\n\nplt.subplot(131)\nplt.plot(wavelength, df_fluorescentF1.illumination, color = 'orange')\n#plt.ylim(-0.05,1.05)\nplt.ylabel('relative intensity', fontsize = 15)\nplt.xlabel('wavelength',fontsize = 15)\n\nplt.subplot(132)\nplt.plot(np.arange(0,len(munsell))+1, explained, 'o-', color = 'black')\nplt.ylabel('Cumulative explained variance ratio', fontsize = 14)\nplt.xlabel('number of principal components', fontsize = 14)\nplt.axvline(x=k+1, color=\"blue\", linewidth = 1)\nplt.axhline(y=accuracy, color=\"red\", linewidth = 1)\n\nplt.subplot(133)\nlabels = ['eigenvector 1', 'eigenvector 2', 'eigenvector 3']\ncolors = ['#00008B', 'Turquoise', '#D2BD96']\nfor i in range(k+1):\n    pc = evecs[i]\n    plt.plot(wavelength, pc, color = colors[i], label = labels[i])\nplt.legend(fontsize = 14)\nplt.xlabel('wavelength',fontsize = 15)\nplt.show()"
  },
  {
    "objectID": "content/hyperspectral_unmixing.html",
    "href": "content/hyperspectral_unmixing.html",
    "title": "Hyperspectral Unmixing",
    "section": "",
    "text": "The rough definition of hyperspectral unmixing is that you have this hyperspectral image such that each pixel of this image is actually a combination of multiple “spectra” or spectral signatures, then basically you are tasked to unmix or untangle these spectra to identify the components or endmembers that make up the image, and their respective abundance. The endmembers are the components of each pixel and the abundance represents the fraction of each endmember in a pixel. In the image below, we see how hyperspectral unmixing works.\n\n\n\nFigure 1: Hyperspectral unmixing concept in the remote sensing applicative context ((Dobigeon et al. 2016))\n\n\nSo in general, there are three steps in hyperspectral unmixing:\n\nEndmember selection\nAbundance estimation\nError minimization"
  },
  {
    "objectID": "content/hyperspectral_unmixing.html#principal-component-analysis",
    "href": "content/hyperspectral_unmixing.html#principal-component-analysis",
    "title": "Hyperspectral Unmixing",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\nFor example, if we have this signal on the right, it is actually composed multiple signals of different forms. And we can perform principal component analysis to identify the signals that form it. We can also set a certain threshold such that we only choose as actual data the first \\(n\\) signals that represent 95% of the data. All signals after the first \\(n\\) signals will now be considered as noise.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.io\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom tqdm import tqdm\npd.options.mode.chained_assignment = None \n%matplotlib inline \n\ndata = scipy.io.loadmat('munsell400_700_5.mat')\n\nmunsell = data['munsell']\nwavelength = np.arange(400,700+5,5)\n\ndf_fluorescent = pd.read_excel(\"Fluorescents.xls\", skiprows = 1)\ndf_fluorescent.rename(columns={'Unnamed: 0':'wavelength'}, inplace=True)\n\ndf_fluorescentF1 = df_fluorescent[['wavelength', 'F1']]\ndf_fluorescentF1.rename(columns = {'F1':'illumination'}, inplace = True)\n\ndf_fluorescentF1 = df_fluorescentF1[(df_fluorescentF1['wavelength'] >= 400) & (df_fluorescentF1['wavelength'] <= 700)]\ndf_fluorescentF1 = df_fluorescentF1.reset_index(inplace=False)\n\ncolor_signal = []\nfor i,j in enumerate(munsell):\n    color_signal.append(j * df_fluorescentF1.illumination[i])\ncolor_signal = np.array(color_signal)\n\npca = PCA(len(color_signal))\npca.fit(color_signal.T)\n\nmean = pca.mean_\nevecs = pca.components_\nevals = pca.explained_variance_ratio_\n\nexplained = pca.explained_variance_ratio_.cumsum() * 100\n\naccuracy = 95\nk = np.argmax(explained>accuracy)\nprint(\"Number of components explaining 95% variance: \"+ str(k+1))\n\nplt.figure(figsize = (9*3,6))\n\nplt.subplot(131)\nplt.plot(wavelength, df_fluorescentF1.illumination, color = 'orange')\n#plt.ylim(-0.05,1.05)\nplt.ylabel('relative intensity', fontsize = 15)\nplt.xlabel('wavelength',fontsize = 15)\n\nplt.subplot(132)\nplt.plot(np.arange(0,len(munsell))+1, explained, 'o-', color = 'black')\nplt.ylabel('Cumulative explained variance ratio', fontsize = 14)\nplt.xlabel('number of principal components', fontsize = 14)\nplt.axvline(x=k+1, color=\"blue\", linewidth = 1)\nplt.axhline(y=accuracy, color=\"red\", linewidth = 1)\n\nplt.subplot(133)\nlabels = ['eigenvector 1', 'eigenvector 2', 'eigenvector 3']\ncolors = ['#00008B', 'Turquoise', '#D2BD96']\nfor i in range(k+1):\n    pc = evecs[i]\n    plt.plot(wavelength, pc, color = colors[i], label = labels[i])\nplt.legend(fontsize = 14)\nplt.xlabel('wavelength',fontsize = 15)\nplt.show()\n\n\nNumber of components explaining 95% variance: 3\n\n\n\n\n\nPitfall of using PCA only: Endmembers cannot be estimated completely just by using PCA. Reason for this is that in real images, the spectral data is nonlinear in nature."
  },
  {
    "objectID": "content/hyperspectral_unmixing.html#endmember-extraction-methods-with-pure-pixels",
    "href": "content/hyperspectral_unmixing.html#endmember-extraction-methods-with-pure-pixels",
    "title": "Hyperspectral Unmixing",
    "section": "Endmember Extraction Methods (with pure pixels)",
    "text": "Endmember Extraction Methods (with pure pixels)\n\nPixel Purity Index (Heylen and Scheunders 2013)\n\n\nFrom what I understood, you can identify which pixels of an image is “pure”. So for example, in an image, a pixel that only contain a cloud particle can be thought of as a “pure cloud pixel”. The existence of these pure pixels and a pure pixel index scores is important in a sense that only pixels whose PPI scores under a certain threshold are considered as endmembers.\nThis algorithm is one of the endmember extraction algorithm made that is still very popular up to this day.\n\n\nN-FINDR Algorithm\n\n\nFor this algorithm, we reduce the dimensionality of the data using princpical component analysis. The number of endmembers is equal to the number of principal components in this algorithm. This algorithm is an iterative approach where the volume of a simplex formed by endmembers are maximized.\n\n\nOrthogonal Subspace Projection (Chang 2005)\n\n\nThe essence of this algorithm is that two spectral bands are separated such that these two bands are as orthogonal to each other. If this is the case, that means one band will not share its information with another band. A requirement for this to work is that the number of spectral bands should be less than the target signature of interest.\n\n\nVertex Component Analysis\n\nPitfall/s: Realistically speaking, pure pixels dont exist. This is why we have algorithms that assumes the absence of pure pixels."
  },
  {
    "objectID": "content/hyperspectral_unmixing.html#endmember-extraction-methods-without-pure-pixels",
    "href": "content/hyperspectral_unmixing.html#endmember-extraction-methods-without-pure-pixels",
    "title": "Hyperspectral Unmixing",
    "section": "Endmember Extraction Methods (without pure pixels)",
    "text": "Endmember Extraction Methods (without pure pixels)\nFor this part, I’m just going to copy what the ParTI website said because I have understood that well and I think they were able to summarize and discuss everything in this part ((N/A 2022)). Other discussions are summarized from multiple papers and online articles I have read trying to understand each algorithms.\nWhen there are no pure pixels algorithms used are going to be minimum volume-based but with different approaches.\n\nSISAL (Simple Identification via variable Splitting and augmented Lagrangian)\n\n\nMinimizes the volume of the simplex that encapsulates the data. It is resilient to noise and outliers - data points can end outside of the simplex. Usually archetype are located outside the data.\nBasically, SISAL is useful in applications where there is a required constrained optimization. From the name itself, SISAL splits the unmixing process into two parts: a variable the represents the abundance of the endmembers in each pixel and a variable that helps hold constraints (non-negative abundance or sum-to-one). Now for the second part of the name, SISAL constructs an augmented Lagragian such that it applies a penalty that is based on the optimization algorithm. This will then be used to guide the optimization process. The algorithm will then iterate based on an error criterion or a set number of iterations. In each iteration, the abundance of each endmember will be calculated and updated based on the penalty and constraints given.\n\n\nMVSA/MVES (Minimum Volume Simplex Analysis/Minimum Volume Enclosing Simplex)\n\n\nMinimizes the volume of the simplex that encapsulates the data. It is not resilient to noise and outliers - data points will all be inside the simplex.\n\n\nSDVMM (Successive Decoupled Volume Max-Min)\n\n\ndata points are selected as archetypes, maximizing the variance of the data encapsulated within the resulting simplex. By Selecting tolerance radius around the points selected as archetypes, one can expand the simplex. In general, the archetypes are located within the data.\n\n\nPCHA (Principal Convex Hull Analysis)\n\n\narchetypes are found within the convex hull of the data, maximizing the data encapsulated within the resulting simplex. By selecting a tolerance radius around the points selected as archetypes, one can expand the simplex. In general, the archetypes are located within the data.\n\nFor large datasets of thousands data points, the three minimal bounding simplex algorithms (SISAL, MVSA and MVES) give very similar results. We found that the SISAL algorithm performs best since it takes good care of outliers in the dataset and gives the tightest simplex enclosing most of the data points. It is then also natural to use SISAL for calculation of the simplex statistical significance.\nHowever, when the number of data points is on the order of a hundred or less, the SISAL algorithm might ignore important points when calculating the enclosing simplex. This is most relevant when calculating the statistical significance of the simplex. In such cases we used the MVSA algorithm.\n\nEndmembers versus archetypes.\nOkay so here, I got confused when I was reading the slides and other papers (Cricri’s SPP paper about voting patterns). So I did more research about the difference of endmembers and archetypes. From what I read, endmembers are these materials in the hyperspectral image, trees, rocks, clouds, etc. These endmembers now have a spectra which we call the endmember signature. Each pixel in our image is then represented by a linear combination of these endmembers with their respective abundance values.\n\\[\n\\text{pixel} = \\text{endmember}_1 * \\text{abundance}_1 + \\text{endmember}_2 * \\text{abundance}_2 + ... + \\text{endmember}_n * \\text{abundance}_n\n\\]\nArchetypes, on the other hand, are the spectral signatures of endmembers. That means they can be a combination of multiple endmembers. For example, one archetype can represent a group of trees. Archetypes are more useful to represent more complex materials with different spectral variatons."
  },
  {
    "objectID": "content/hyperspectral_unmixing.html#abundance-estimation-and-error-minimization",
    "href": "content/hyperspectral_unmixing.html#abundance-estimation-and-error-minimization",
    "title": "Hyperspectral Unmixing",
    "section": "Abundance Estimation and Error Minimization",
    "text": "Abundance Estimation and Error Minimization\nAfter identifying the endmembers of our image, next thing to do is to estimate the fraction of endmembers in each pixel, quantified by abundance. One of the easiest ways to estimate abundance is by least squares. When calculating the abundance, the sum-to-one and non-negative constraints should be considered when calculating the least squares. Now, the process in estimating abundance is iterative such that the reconstruction error (obtained using the endmembers and abundance estimated) is minimized when compared to the original hyperspectral image."
  },
  {
    "objectID": "content/hyperspectral_unmixing.html#applications-of-hyperspectral-unmixing",
    "href": "content/hyperspectral_unmixing.html#applications-of-hyperspectral-unmixing",
    "title": "Hyperspectral Unmixing",
    "section": "Applications of Hyperspectral Unmixing",
    "text": "Applications of Hyperspectral Unmixing\nOne of the cool things that we can use hyperspectral unmixing for is to discover voting patterns of people using an election dataset (Castro and Lim 2020). In the paper, they were able to identify the archetypes in an election dataset: that is the voting distribution of people. They found recurring archetypes across different election periods. With this paper, we see that hyperspectral unmixing can also be used to different types of data as long as these data contain some sort of “components” that can be unmixed."
  },
  {
    "objectID": "content/language-model.html",
    "href": "content/language-model.html",
    "title": "Language Models",
    "section": "",
    "text": "Language models are an important part of natural language processing (NLP) since they provide a way for computers to process or understand the natural language and to be able to extract information that can be used in different applications. These natural language are something that is being used by humans and evolves naturally through repetitive use (speech, sign language, non-verbal cues)."
  },
  {
    "objectID": "content/language-model.html#what-is-a-language-model",
    "href": "content/language-model.html#what-is-a-language-model",
    "title": "Language Models",
    "section": "What is a language model?",
    "text": "What is a language model?\nIn its simplest form, a language model is a probability distribution such that, given a sequence of words, a language model will give the probability of all the words being chosen next in the available vocabulary. For example, given a series of words \\(w_{1:(t-1)} = (w_1,...,w_{t-1})\\), a language model will give us the probability of all words in its vocabulary \\(V\\) to appear next to the series of words. \\[\nP(w_t | w_{1:(t-1)}), \\ \\ \\ \\ \\ \\ \\ w_1,...,w_{t-1}\\in V\n\\]\nSo if we want our model to complete our sentence, we select the word which has the highest probability and then put that word to the series of words and then use the model again to generate the next word. So the essence of a language model is to simply imitate how a human would respond to different prompts.\nTo give an idea of what a language model can do, let us look at the ChatGPT, see Figure 1, which is an artificial intelligence (AI) chatbot that is developed by OpenAI last November 2022.\n\n\n\nFigure 1: Example of a language model (ChatGPT)\n\n\nWe asked the AI to explain quantum computing in simple terms and we see how it was able to provide an extensive discsussion of what quantum computing is. Now, the reason why these AI chatbots are gaining such traction is that it serves as a good starting point when writing or when you want to gain a better understanding of topics that were not discussed properly for example."
  },
  {
    "objectID": "content/language-model.html#neural-network-based-language-model-types-of-language-models",
    "href": "content/language-model.html#neural-network-based-language-model-types-of-language-models",
    "title": "Language Models",
    "section": "Neural Network-Based Language Model (Types of Language Models)",
    "text": "Neural Network-Based Language Model (Types of Language Models)\nChatGPT is an example of a generative pre-trained transformer (GPT) which is a type of language model that relies on deep learning that generates texts based on a given input text."
  },
  {
    "objectID": "content/language-model.html#training-language-models",
    "href": "content/language-model.html#training-language-models",
    "title": "Language Models",
    "section": "Training Language Models",
    "text": "Training Language Models\nFocusing on the transformer models, we need to know how the neural network model is being trained: including the pre-training and fine-tuning processes. In pre-training process, we provide a general language model that has a good understarding of how language is being used in a variety of settings.\nFor a neural network, we use \\(n\\) inputs and then it is processed in several hidden layers until it reaches an output layer. Ideally, we want the output of our neural network to be close to the actual/real output. If that is not the case, then we use gradient descent to change the parameters to reduce the loss function of our network. In gradient descent, what happens is we use the output and compute some values and then feed it back to our hidden layers until the loss is minimized, which is why the process is also called backpropagation, see Figure 2.\n\n\n\nFigure 2: Simplified Neural Network\n\n\nOne problem of the neural network above is that the input size is fixed and in general, we would want to process input sizes that are longer or shorter. Now in the field of language moelling, there are two types of networks that were considered state of the art: recurrent neural networks and long short-term memory networks.\n\nRecurrent neural networks (RNNs)\nIn a recurrent neural network, we still have the same neural network discussed above to every word in a series of words. Whats different for RNNs is that last word (newest word) in the sequence of words has the most influence in choosing the next word and the probability of influence of previous words reduce exponentially as new words are being introduced. We see in Figure 3 how RNNs are able to connect information (or words) in a sequential manner.\n\n\n\nFigure 3: Recurring Neural Network\n\n\nThis makes sense but the problem is, language in itself is more nuanced in the sense that sometimes we need to take into account not only the last word in our sentence but the sentence as a whole. For example, in Figure 3, if we need the output \\(h_{10}\\), the information from the input \\(x_1\\) has very little effect on the output and this might pose a problem if we are for example dealing with a sentence whose subject and verb are very far from each other. Because of this unique feature of language, the concept of older worlds having less influence becomes a bug and is called the vanishing gradients problem\n\n\nLong short-term memory (LSTM) networks\nNow LSTMs solve the vanishing gradients problem by introducing a “memory” state whose influence is determined by gates defined by more learnable parameters. The main difference of LSTM fromm RNN is that the former type of network remembers information for long periods of time by default but has there functions, called gates that can either use the information stored to process the output or to “forget” the previous information and not use for the output.\n\n\n\nFigure 4: Long Short-Term Memory Network\n\n\nIn an LSTM network, there are three gates that are needed to pass through. First is the gate that chooses which information to forget, then the second gate decides which information to store. In the second gate, we have two functions which decides which values will be updated and what values will be used to update. Lastly, the third gate will decide which information will be the output.\nDrawback of RRN and LSTM Now, while both networks are successful in predicting the next word given a series of words, one major drawback of these two networks is that they require their input data to be processed sequentially, that is in order to process the next word of input \\(x_i\\), we need the result of the previous input \\(x_{i-1}\\).\nThe attention mechanism addresses this issue by stacking the input in a matrix such that they can all be processed at the same time. According to the paper “Attention is all you need”, the authors defined the attention function as the following:\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum  of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nNow, the attention mechanism is used in a neural network architecture called a transformer.\nHere is how the transformer neural network works:\n\nTokenization\n\nTokenization is the process of converting words or any natural language to tokens which are the ones processed by our computers. There are three main approaches in tokenization which are the following:\n\nWord-based: Split a sentence on spaces where generally, punctuation marks are also split into different tokens.\nSubword-based: Split words into subwords. Example would be “o c ca sion”.\nCharacter-based: Split sentence into individual characters.\n\nFor example, taking the sentence “Once again Mr. Costner has dragged out a movie for far longer than necessary.”\nUsing the fastai module in Python, we can implement the different types of tokenization. For example, for word tokenization, we have the following code and output\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\ntxt = files[0].open().read()\nprint(\"Original Sentence:\")\nprint(txt[:77])\nprint()\nspacy = WordTokenizer()\ntoks = first(spacy([txt[:77]]))\nprint(\"Word Tokenization:\")\nprint(coll_repr(toks, 30))\n\n\nOriginal Sentence:\nOnce again Mr. Costner has dragged out a movie for far longer than necessary.\n\n\n\nWord Tokenization:\n(#15) ['Once','again','Mr.','Costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.']\n\n\nTokenization is an important step in the neural network architecture since it converts the natural language that we understand to these tokens that can be understood by our machines after converter them to numbers, a process called Numericalization which can also be done using the fastai module.\nNow that we have the numbers that our computer can process, we can actually now split them into 3 batches: training, validating, and testing batches. We put the training batch in a chosen language model and then validate the results. Now after training our model, we can now actually specify the things that our model can do. In the following chapter, we discuss some of the things that language models can actually do."
  },
  {
    "objectID": "content/language-model.html#different-things-models-for-nlp-can-do",
    "href": "content/language-model.html#different-things-models-for-nlp-can-do",
    "title": "Language Models",
    "section": "Different things models for NLP can do!",
    "text": "Different things models for NLP can do!\nText Classification\nThere are two types of classification wherein NLP models are very useful for: binary classification and multi-label classification. Binary classification is useful in sentiment analysis where texts can be labeled as something good or bad which can help informed decisions in fields like politics, and marketing. Multi-label classification on the other hand is useful in organizing and filtering tons of information in social media and news for example.\nToken Classification\nIn token classification, we classify the tokens produced after the tokenization process. This is actually useful for example in in labeling tokens specifically those that can be categorized as people or entities that need to be identified. Another thing that this is useful for is for identifying which part of speech certain words/tokens are. We can actually use NLP models to create those tree diagrams where a sentence can be broken down into different part of speeches!\nTranslation\nOf course, a common application of NLP models is for translating a language to another. Language models learns the structure of the input so that it can be able to translate it to another language with the use of transfer learning models. These transfer learning models are actually useful in order to reduce the time it would take the model to train.\n\nQuestion Answering\nNow, one of the most commonly seen application of language models is this question answering. Siri, Alexa, Bixby, and Google Assistant are examples of some end-user products that uses language model to answer day to day questions of people. And because of the sudden interest of the general public to these language models (ChatGPT for example), many researchers and developers have actually made their pre-trained models open access, which means they are publicly available for non-commercial use. An example of this would be the alpaca.cpp which makes use of the LLaMA foundation model with some finetuning mechanisms and a couple tweaks to make it into a chat interface. Here is a sample gif of how it works. It’s basically a locally run ChatGPT!\n\n\n\nFigure 5: Example of alpaca.cpp"
  },
  {
    "objectID": "content/language_model.html",
    "href": "content/language_model.html",
    "title": "Physics 312 - Advanced Mathematical Physics II",
    "section": "",
    "text": "Language models are an important part of natural language processing (NLP) since they provide a way for computers to process or understand the natural language and to be able to extract information that can be used in different applications. These natural language are something that is being used by humans and evolves naturally through repetitive use (speech, sign language, non-verbal cues)."
  },
  {
    "objectID": "content/language_model.html#what-is-a-language-model",
    "href": "content/language_model.html#what-is-a-language-model",
    "title": "Physics 312 - Advanced Mathematical Physics II",
    "section": "What is a language model?",
    "text": "What is a language model?\nIn its simplest form, a language model is a probability distribution such that, given a sequence of words, a language model will give the probability of all the words being chosen next in the available vocabulary. For example, given a series of words \\(w_{1:(t-1)} = (w_1,...,w_{t-1})\\), a language model will give us the probability of all words in its vocabulary \\(V\\) to appear next to the series of words. \\[\nP(w_t | w_{1:(t-1)}), \\ \\ \\ \\ \\ \\ \\ w_1,...,w_{t-1}\\in V\n\\]\nSo if we want our model to complete our sentence, we select the word which has the highest probability and then put that word to the series of words and then use the model again to generate the next word. So the essence of a language model is to simply imitate how a human would respond to different prompts.\nTo give an idea of what a language model can do, let us look at the ChatGPT, see Figure 1, which is an artificial intelligence (AI) chatbot that is developed by OpenAI last November 2022.\n\n\n\nFigure 1: Example of a language model (ChatGPT)\n\n\nWe asked the AI to explain quantum computing in simple terms and we see how it was able to provide an extensive discsussion of what quantum computing is. Now, the reason why these AI chatbots are gaining such traction is that it serves as a good starting point when writing or when you want to gain a better understanding of topics that were not discussed properly for example."
  },
  {
    "objectID": "content/language_model.html#neural-network-based-language-model-types-of-language-models",
    "href": "content/language_model.html#neural-network-based-language-model-types-of-language-models",
    "title": "Physics 312 - Advanced Mathematical Physics II",
    "section": "Neural Network-Based Language Model (Types of Language Models)",
    "text": "Neural Network-Based Language Model (Types of Language Models)\nChatGPT is an example of a generative pre-trained transformer (GPT) which is a type of language model that relies on deep learning that generates texts based on a given input text."
  },
  {
    "objectID": "content/language_model.html#training-language-models",
    "href": "content/language_model.html#training-language-models",
    "title": "Physics 312 - Advanced Mathematical Physics II",
    "section": "Training Language Models",
    "text": "Training Language Models\nFocusing on the transformer models, we need to know how the neural network model is being trained: including the pre-training and fine-tuning processes. In pre-training process, we provide a general language model that has a good understarding of how language is being used in a variety of settings.\nFor a neural network, we use \\(n\\) inputs and then it is processed in several hidden layers until it reaches an output layer. Ideally, we want the output of our neural network to be close to the actual/real output. If that is not the case, then we use gradient descent to change the parameters to reduce the loss function of our network. In gradient descent, what happens is we use the output and compute some values and then feed it back to our hidden layers until the loss is minimized, which is why the process is also called backpropagation, see Figure 2.\n\n\n\nFigure 2: Simplified Neural Network\n\n\nOne problem of the neural network above is that the input size is fixed and in general, we would want to process input sizes that are longer or shorter. Now in the field of language moelling, there are two types of networks that were considered state of the art: recurrent neural networks and long short-term memory networks.\n\nRecurrent neural networks (RNNs)\nIn a recurrent neural network, we still have the same neural network discussed above to every word in a series of words. Whats different for RNNs is that last word (newest word) in the sequence of words has the most influence in choosing the next word and the probability of influence of previous words reduce exponentially as new words are being introduced. We see in Figure 3 how RNNs are able to connect information (or words) in a sequential manner.\n\n\n\nFigure 3: Recurring Neural Network\n\n\nThis makes sense but the problem is, language in itself is more nuanced in the sense that sometimes we need to take into account not only the last word in our sentence but the sentence as a whole. For example, in Figure 3, if we need the output \\(h_{10}\\), the information from the input \\(x_1\\) has very little effect on the output and this might pose a problem if we are for example dealing with a sentence whose subject and verb are very far from each other. Because of this unique feature of language, the concept of older worlds having less influence becomes a bug and is called the vanishing gradients problem\n\n\nLong short-term memory (LSTM) networks\nNow LSTMs solve the vanishing gradients problem by introducing a “memory” state whose influence is determined by gates defined by more learnable parameters. The main difference of LSTM fromm RNN is that the former type of network remembers information for long periods of time by default but has there functions, called gates that can either use the information stored to process the output or to “forget” the previous information and not use for the output.\n\n\n\nFigure 4: Long Short-Term Memory Network\n\n\nIn an LSTM network, there are three gates that are needed to pass through. First is the gate that chooses which information to forget, then the second gate decides which information to store. In the second gate, we have two functions which decides which values will be updated and what values will be used to update. Lastly, the third gate will decide which information will be the output.\nDrawback of RRN and LSTM Now, while both networks are successful in predicting the next word given a series of words, one major drawback of these two networks is that they require their input data to be processed sequentially, that is in order to process the next word of input \\(x_i\\), we need the result of the previous input \\(x_{i-1}\\).\nThe attention mechanism addresses this issue by stacking the input in a matrix such that they can all be processed at the same time. According to the paper “Attention is all you need”, the authors defined the attention function as the following:\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum  of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nNow, the attention mechanism is used in a neural network architecture called a transformer.\nLet us now discuss how transformer neural network works. First step is what we call tokenization. Tokenization is the process of converting words or any natural language to tokens which are the ones processed by our computers. There are three main approaches in tokenization which are the following:\n\nWord-based: Split a sentence on spaces where generally, punctuation marks are also split into different tokens.\nSubword-based: Split words into subwords. Example would be “o c ca sion”.\nCharacter-based: Split sentence into individual characters.\n\nFor example, taking the sentence “Once again Mr. Costner has dragged out a movie for far longer than necessary.”\nUsing the fastai module in Python, we can implement the following types of tokenization. For the word-based tokenization, we use the following block of following code:\n\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n\n\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n\n\ntxt = files[0].open().read(); txt[:77]\n\n'Once again Mr. Costner has dragged out a movie for far longer than necessary.'\n\n\n\nspacy = WordTokenizer()\ntoks = first(spacy([txt[:77]]))\nprint(coll_repr(toks, 30))\n\n(#15) ['Once','again','Mr.','Costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.']\n\n\nFor subword tokenization:\n\ntxts = L(o.open().read() for o in files[:2000])\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\nsubword(1000)\n\n\n\n\n\n\n\n\n'▁O n ce ▁again ▁M r . ▁Co st n er ▁has ▁ d ra g g ed ▁out ▁a ▁movie ▁for ▁far ▁long er ▁than ▁ ne ce s s ar y . ▁A side ▁from ▁the ▁ ter'\n\n\nTokenization is an important step in the neural network architecture since it converts the natural language that we understand to these tokens that can be understood by our machines after converter them to numbers, a process called Numericalization which can also be done using the fastai module.\nNow that we have the numbers that our computer can process, we can actually now split them into 3 batches: training, validating, and testing batches. We put the training batch in a chosen language model and then validate the results. Now after training our model, we can now actually specify the things that our model can do. In the following chapter, we discuss some of the things that language models can actually do.\n\n\nDifferent things models for NLP can do!\nText Classification\nThere are two types of classification wherein NLP models are very useful for: binary classification and multi-label classification. Binary classification is useful in sentiment analysis where texts can be labeled as something good or bad which can help informed decisions in fields like politics, and marketing. Multi-label classification on the other hand is useful in organizing and filtering tons of information in social media and news for example.\nToken Classification\nIn token classification, we classify the tokens produced after the tokenization process. This is actually useful for example in in labeling tokens specifically those that can be categorized as people or entities that need to be identified. Another thing that this is useful for is for identifying which part of speech certain words/tokens are. We can actually use NLP models to create those tree diagrams where a sentence can be broken down into different part of speeches!\nTranslation\nOf course, a common application of NLP models is for translating a language to another. Language models learns the structure of the input so that it can be able to translate it to another language with the use of transfer learning models. These transfer learning models are actually useful in order to reduce the time it would take the model to train.\nQuestion Answering\nNow, one of the most commonly seen application of language models is this question answering. Siri, Alexa, Bixby, and Google Assistant are examples of some end-user products that uses language model to answer day to day questions of people. And because of the sudden interest of the general public to these language models (ChatGPT for example), many researchers and developers have actually made their pre-trained models open access, which means they are publicly available for non-commercial use. An example of this would be the alpaca.cpp which makes use of the LLaMA foundation model with some finetuning mechanisms and a couple tweaks to make it into a chat interface. Here is a sample gif of how it works. It’s basically a locally run ChatGPT!\n\n\n\nFigure 5: Example of alpaca.cpp"
  },
  {
    "objectID": "content/nlp_beginners.html",
    "href": "content/nlp_beginners.html",
    "title": "Physics 312 - Advanced Mathematical Physics II",
    "section": "",
    "text": "from fastai.text.all import *\npath = untar_data(URLs.IMDB)\n\nModuleNotFoundError: No module named 'fastai'\n\n\n\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n\n\ntxt = files[0].open().read(); txt[:77]\n\n'Once again Mr. Costner has dragged out a movie for far longer than necessary.'\n\n\nWord Tokenization\n\n\nCode\nspacy = WordTokenizer()\ntoks = first(spacy([txt[:77]]))\nprint(coll_repr(toks, 30))\n\n\n(#15) ['Once','again','Mr.','Costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.']\n\n\n\nfirst(spacy(['The U.S. dollar $1 is $1.00.']))\n\n(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']\n\n\n\ntkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 31))\n\n(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]\n\n\n\ndefaults.text_proc_rules\n\n[<function fastai.text.core.fix_html(x)>,\n <function fastai.text.core.replace_rep(t)>,\n <function fastai.text.core.replace_wrep(t)>,\n <function fastai.text.core.spec_add_spaces(t)>,\n <function fastai.text.core.rm_useless_spaces(t)>,\n <function fastai.text.core.replace_all_caps(t)>,\n <function fastai.text.core.replace_maj(t)>,\n <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]\n\n\n\ncoll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)\n\n\"(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\"\n\n\nSubword Tokenization\n\ntxts = L(o.open().read() for o in files[:2000])\n\n\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n\n\nsubword(1000)\n\n'▁O n ce ▁again ▁M r . ▁Co st n er ▁has ▁ d ra g g ed ▁out ▁a ▁movie ▁for ▁far ▁long er ▁than ▁ ne ce s s ar y . ▁A side ▁from ▁the ▁ ter'\n\n\n\nsubword(200)\n\n'▁ O n ce ▁a g a in ▁ M r . ▁ C o s t n er ▁ h a s ▁ d ra g g ed ▁ o u t ▁a ▁movie ▁for ▁f ar ▁ l'\n\n\n\nsubword(10000)\n\n'▁On ce ▁again ▁Mr . ▁Costner ▁has ▁dragged ▁out ▁a ▁movie ▁for ▁far ▁longer ▁than ▁necessary . ▁A side ▁from ▁the ▁terrific ▁sea ▁rescue ▁sequences , ▁of ▁which ▁there ▁are ▁very ▁few ▁I ▁just ▁did ▁not ▁care ▁about ▁any ▁of'\n\n\nNumericalization with fastai\n\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))\n\n(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has','dragged','out','a','movie','for','far','longer','than','necessary','.','xxmaj','aside','from','the','terrific','sea','rescue','sequences',',','of','which'...]\n\n\n\ntoks200 = txts[:200].map(tkn)\ntoks200[0]\n\n(#207) ['xxbos','xxmaj','once','again','xxmaj','mr','.','xxmaj','costner','has'...]\n\n\n\nnum = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab,20)\n\n\"(#1968) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','it','i','in'...]\"\n\n\n\nnums = num(toks)[:20]; nums\n\nTensorText([   2,    8,  349,  183,    8, 1177,   10,    8, 1178,   60, 1455,\n              62,   12,   25,   28,  189,  957,   93,  958,   10])\n\n\n\n' '.join(num.vocab[o] for o in nums)\n\n'xxbos xxmaj once again xxmaj mr . xxmaj costner has dragged out a movie for far longer than necessary .'\n\n\nPutting Our Texts into Batches for a Language Model\n\nfrom weasyprint import HTML\n\n#hide_input\nstream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\ntokens = tkn(stream)\nbs,seq_len = 6,15\nd_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))\n\n\n-----\n\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting \n\n-----\n\n\n\nOSError: cannot load library 'gobject-2.0-0': error 0x7e.  Additionally, ctypes.util.find_library() did not manage to locate a library called 'gobject-2.0-0'"
  },
  {
    "objectID": "content/penguins.html",
    "href": "content/penguins.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "import pandas as pd\nimport altair as alt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nData from Palmer Penguins R package\n\npenguins = pd.read_csv(\"https://pos.it/palmer-penguins-github-csv\")\n\n\npenguins.groupby(\"species\").size().reset_index(name = \"count\")\n\n\n\n\n\n  \n    \n      \n      species\n      count\n    \n  \n  \n    \n      0\n      Adelie\n      152\n    \n    \n      1\n      Chinstrap\n      68\n    \n    \n      2\n      Gentoo\n      124\n    \n  \n\n\n\n\n\ncolors = [\"#FF8C00\", \"#A020F0\", \"#008B8B\"]\nsns.set_palette(colors, n_colors = 3)\n\n\npenguins[\"bill_ratio\"] = (\n   penguins[\"bill_length_mm\"] / penguins[\"bill_depth_mm\"] \n)\nsns.displot(penguins, \n            x = \"bill_ratio\", \n            hue = \"species\", \n            kind = \"kde\", fill = True, aspect = 2, height = 3)\nplt.show()\n\n\n\n\n\nsns.displot(penguins, \n            x = \"bill_depth_mm\", \n            hue = \"species\", \n            kind = \"kde\", fill = True, \n            aspect = 2, height = 3)\nplt.show()\nsns.displot(penguins, \n            x = \"bill_length_mm\", \n            hue = \"species\", \n            kind = \"kde\", fill = True, \n            aspect = 2, height = 3)\nplt.show()\n\n\n\n\n\n\n\n(a) Gentoo penguins tend to have thinner bills,\n\n\n\n\n\n\n\n(b) and Adelie penguins tend to have shorter bills.\n\n\n\n\nFigure 1: Marginal distributions of bill dimensions\n\n\n\n\nscale = alt.Scale(domain = ['Adelie', 'Chinstrap', 'Gentoo'],\n                  range = colors)\n\n\nalt.Chart(penguins).mark_circle(size=60).encode(\n    alt.X('bill_length_mm',\n        scale=alt.Scale(zero=False)\n    ),\n    alt.Y('bill_depth_mm',\n        scale=alt.Scale(zero=False)\n    ),\n    color = alt.Color('species', scale = scale),\n    tooltip=['species', 'sex', 'island']\n)\n\n\n\n\n\nFigure 2: A scatterplot of bill dimensions for penguins, made with Altair."
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings > Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings > Actions > General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings > Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File > New Project and then select “Version Control”. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the “Build” tab. Click the “Render website” button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify “preview in browser” or set up RStudio to preview to the viewer panel. To do the latter, go to Tools > Global Options > R Markdown. Then select “Show output preview in: Viewer panel”."
  },
  {
    "objectID": "content/report.html",
    "href": "content/report.html",
    "title": "Report",
    "section": "",
    "text": "For my report, I decided to use the alpaca.cpp and try to explore it a little bit more. Specifically, we look at the different parameters that were used in this language model. For example, temperature is a hyper-parameter used in language model and is often used to control smoothness of a probability distribution of predicted words (Wang et al. 2020). When we have a temperature approaching infinity, the distribution becomes uniform and when temperature approaches zero, we get a dirac-delta looking distribution. We can think of is as if we have high temperature, the answers that our chat AI gives becomes varied uniform, but if we have a low temperature, we get one answer almost, if not all, the time."
  },
  {
    "objectID": "content/report.html#default-parameters-of-alpaca.cpp",
    "href": "content/report.html#default-parameters-of-alpaca.cpp",
    "title": "Report",
    "section": "Default parameters of alpaca.cpp",
    "text": "Default parameters of alpaca.cpp\nHere are the parameters that we can actually control in the alpaca.cpp language model. We see that there are actually multiple hyper-parameters that we can try to vary and study its effects. For this report, let us just look at the effects of few parameters.\n  -s SEED, --seed SEED  RNG seed (default: -1)\n  -t N, --threads N     number of threads to use during computation (default: 4)\n  -p PROMPT, --prompt PROMPT\n                        prompt to start generation with (default: random)\n  -f FNAME, --file FNAME\n                        prompt file to start generation.\n  -n N, --n_predict N   number of tokens to predict (default: 128)\n  --top_k N             top-k sampling (default: 40)\n  --top_p N             top-p sampling (default: 0.9)\n  --repeat_last_n N     last n tokens to consider for penalize (default: 64)\n  --repeat_penalty N    penalize repeat sequence of tokens (default: 1.3)\n  -c N, --ctx_size N    size of the prompt context (default: 2048)\n  --temp N              temperature (default: 0.1)\n  -b N, --batch_size N  batch size for prompt processing (default: 8)\n  -m FNAME, --model FNAME\n                        model path (default: ggml-alpaca-7b-q4.bin)"
  },
  {
    "objectID": "content/report.html#varying-the-temperature",
    "href": "content/report.html#varying-the-temperature",
    "title": "Report",
    "section": "Varying the temperature",
    "text": "Varying the temperature\nUsing a low value (temp = 0.01) and a high value (temp = 0.9) for temperature , let us ask alpaca.cpp to describe the color red in three sentences (Thank you ChatGPT for the prompt!). Here are the first five outputs of the model.\n\nDescribe the color red in three sentences. {tbl-colwidths=“[50,50]”}\n\n\n\n\n\n\n\nTemperature = 0.01\nTemperature = 0.9\n\n\n\n\nThe color red is often associated with energy, passion and intensity. It can be a bold choice for decorating an entire room or simply adding pop of color to accessories like throw pillows or rugs. Its warmth makes it perfect for creating cozy spaces that invite comfort and relaxation.\nThe color red is often associated with energy and vitality. Its intensity can be both invigorating and intimidating at times - it’s a striking hue that makes an indelible statement wherever it is used. Red has long been known to symbolize courage, strength and determination; qualities we admire in people as well as objects around us.\n\n\nThe color red is often associated with energy, passion and intensity. It can be a bold choice for decorating an entire room or simply adding pop of color to accessories like throw pillows or rugs. Its warmth makes it perfect for creating cozy spaces that invite comfort and relaxation.\nRed is a vibrant hue with strong associations - it’s often seen as an expression of passion, energy and courage. In some cultures, it can also symbolize power, strength, and determination. Its warmth makes it ideal for creating a cozy atmosphere in homes or restaurants, while its intensity lends itself to making statements when used on clothing or products.\n\n\nThe color red is often associated with energy, passion and intensity. It can be a bold choice for decorating an entire room or simply adding pop to accessories like throw pillows or rugs. Its warmth makes it perfect for creating cozy spaces that invite comfort and relaxation.\nRed is often seen as an expresison of passion and energy due to its warmth and vibrancy. It’s also associated with courage, strenght, determiantion and a few other things related to raw power and emotion. Red can be used on clothing and products to make statements about the person wearing it (or using them) - whether they are bold, confident, daring and so much more!\n\n\nThe color red is often associated with energy, passion and intensity. It can be a bold choice for decorating an entire room or simply adding pop to accessories like throw pillows or rugs. Its warmth makes it perfect for creating cozy spaces that invite comfort and relaxation.\nThe color red has associations with energy, passion and intensity. It can evoke feelings of warmth and excitement as well as alertness and danger. Red is a vibrant ue that stands out from other colors due to its brightness and intensity.\n\n\nThe color red is often associated with energy, passion and intensity. It can be a bold choice for decorating an entire room or simply adding pop to accessories like throw pillows or rugs. Its warmth makes it perfect for creating cozy spaces that invite comfort and relaxation.\nRed is often associated with energy, passion, and excitement. It’s also known for being attention-grabbing and can evoke strong emotions depending on how it’s used. Red evokes warmth and a sense of vibrancy that makes it the percent accent color to any design.\n\n\n\nWe see from the table above, just from the first five results the difference in the results of alpaca.cpp language model if we vary the temperature. We see that for low temperature results, the results are repetitive and that the sentences formed are not that ‘creative’. On the other hand, the results produced by high temperature are more varied and more creative - that is the sentences formed are more “imaginatively” written.\nKey idea: We see here that a balance of temperature will basically control the level of ‘randomness’ of the results. Now, to make this study more quantitative, its nice to make a measure of creativity."
  },
  {
    "objectID": "content/report.html#varying-the-top-k-and-top-p",
    "href": "content/report.html#varying-the-top-k-and-top-p",
    "title": "Report",
    "section": "Varying the top-k and top-p",
    "text": "Varying the top-k and top-p\nNow there are two parameters that can also greatly affect the ouput of our language model and these are the top_k and top_p parameters. So given a list of possible next words in a fill in the blank type of sentence, ordered from highest to lowest probability of being chosen, the top_k and top_p parameters control the ‘cut-off’ of which words will be considered in the final list of possible words (Holtzman et al. 2020).\nI think this is better to be presented using visuals. Suppose we have the prompt “The weather today is ___” and we have 5 possible next words, ordered in decreasing probability of being the actual next word.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nweather_conditions = ['Sunny', 'Cloudy', 'Rainy', 'Windy', 'Snowy']\nfrequency = [10, 8, 5, 3, 2]\nfrequency_norm = np.array(frequency)/sum(frequency)\n\nsorted_data = sorted(zip(frequency_norm * 100, weather_conditions))\n\nsorted_frequency, sorted_weather_conditions = zip(*sorted_data)\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.barh(sorted_weather_conditions, sorted_frequency, color = 'blue')\n\nax.set_xlabel('Probability (%)')\nax.set_ylabel('Possible next word')\nax.set_title('The weather today is _____')\n\nplt.show()\n\n\n\n\n\nNow if we set top_k = 2, that means that we will only consider the top 2 words which are Sunny and Cloudy. A top_k = 1 value means that the language model deploys some sort of greedy algorithm where only the top answer is considered as the best answer. On the other hand, if we set top_p = 0.8, that means that we are only considering the words whose sum of probabilities are greater than the set value of top_p, which in our case are Sunny, Cloudy and Rainy.\n\n\nCode\nweather_conditions = ['Sunny', 'Cloudy', 'Rainy', 'Windy', 'Snowy']\nfrequency = [10, 8, 5, 3, 2]\nfrequency_norm = np.array(frequency)/sum(frequency)\nc_k = ['red', 'red', 'blue', 'blue', 'blue']\nc_p = ['red', 'red', 'red', 'blue', 'blue']\n\nsorted_data = sorted(zip(frequency_norm * 100, weather_conditions))\n\nsorted_frequency, sorted_weather_conditions = zip(*sorted_data)\n\nfig, (ax1, ax2)  = plt.subplots(1, 2, figsize=(10, 5))\nax1.barh(sorted_weather_conditions, sorted_frequency, color = np.array(c_k)[::-1])\n\nax1.set_xlabel('Probability')\nax1.set_ylabel('Possible next word')\nax1.set_title('$top_k = 2')\n\nax2.barh(sorted_weather_conditions, sorted_frequency, color = c_p[::-1])\n\nax2.set_xlabel('Probability')\n\nplt.show()\n\n\n\n\n\nSo we see from the discussion above that aside from temperature, top-k and top-p values are also need to be considered when tweaking the language model."
  }
]